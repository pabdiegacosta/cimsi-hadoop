---
# General
# Definición de usuario, grupo y contraseña así como el directorio home
user: "hadoop"
group: "sudo"
home_dir: "/home/{{ user }}"
pass: "cimsi"


# Ansible
# Contraseña para la conexión SSH y become de Ansible
ansible_user: "{{ user }}"
ansible_ssh_pass: "{{ pass }}"
ansible_become_pass: "{{ pass }}"
ansible_ssh_common_args: "-o StrictHostKeyChecking=no"


# Haddoop installation
# Definición de la versión de Hadoop, enlace de descarga y directorio de instalación
hadoop_tar_filename: "hadoop-3.3.5"
hadoop_download_link: "https://downloads.apache.org/hadoop/common/{{ hadoop_tar_filename }}/{{ hadoop_tar_filename }}.tar.gz"
hadoop_tar_path: "{{ home_dir }}/{{ hadoop_tar_filename }}.tar.gz"
hadoop_install_path: "{{ home_dir }}"

# Hadoop configurations
# Usuario y grupo de Hadoop, directorio home de instalación de Hadoop
hadoop_user: "{{ user }}"
hadoop_group: "{{ group }}"
hadoop_home: "{{ hadoop_install_path }}/hadoop"
# Configuration, logs and pid directories
hadoop_conf_dir: "{{ hadoop_home }}/etc/hadoop" #hdfs_hadoop_conf_dir: "/opt/hadoop/etc/hadoop"
hadoop_log_dir: "{{ home_dir }}/log/hadoop" #hdfs_hadoop_log_dir: "/var/log/hadoop"
hadoop_pid_dir: "{{ home_dir }}/run/hadoop"
# Other Hadoop and YARN environment variables (hadoop-env.sh)
java_home: "/usr/lib/jvm/java-8-openjdk-amd64" #hdfs_java_home: "/usr"
hadoop_heapsize: 2048
hadoop_namenode_opts: "-Xmx2048m"
yarn_resourcemanager_opts: "-Xmx1024m"
hadoop_datanode_opts: "-Xmx1024m"
yarn_nodemanager_opts: "-Xmx512m"

# HDFS configurations
# HDFS namenode host:port and user
hdfs_namenode_host: "{{ groups['master'][0] }}"
hdfs_namenode_port: 9000
hdfs_namenode_user: "{{ hadoop_user}}"
# HDFS datanode user
hdfs_datanode_user: "hadoop"
# HDFS directories
hdfs_name_dir: "{{ home_dir }}/hdfs/name"
hdfs_data_dir: "{{ home_dir }}/hdfs/data"
hdfs_temp_dir: "{{ home_dir }}/hdfs/tmp"
# Replication and failover (no configurado en hdf-site.xml)
hdfs_dfs_replication: 2
hdfs_ha_automatic_failover_enabled: "true"

# Spark Installation
# Definición de la versión de Spark, enlace de descarga y directorio de instal>
spark_tar_filename: "spark-3.5.3-bin-hadoop3"
spark_download_link: "https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz"
spark_tar_path: "{{ home_dir }}/{{ spark_tar_filename }}.tgz"
spark_install_path: "{{ home_dir }}"

# Spark configurations
# Directorio home de instalación de Spark
spark_home: "{{ hadoop_install_path }}/spark"

# mapred-site.xml
yarn_app_mapreduce_am_resource_mb: 1024
mapreduce_map_memory_mb: 512
mapreduce_reduce_memory_mb: 512

# yarn-site.xml
yarn_nodemanager_resource_memory_mb: 3072
yarn_scheduler_maximum_allocation_mb: 3072
yarn_scheduler_minimum_allocation_mb: 256
yarn_nodemanager_vmem_check_enabled: "false"

# hdfs input files upload & park job
hdfs_input_path: "/user/{{ user }}/word2vec/input"
input_file: "text2.txt"
hdfs_output_path: "/user/{{ user }}/word2vec/output"
vector_size: 2
num_partitions: 4
spark_master: "yarn"
deploy_mode: "cluster"
executor_memory: "4G"
spark_driver_memory: '1G'
spark_executor_memory: '2304m'
spark_yarn_executor_memoryOverhead: '512m'
executor_cores: 1
num_executors: 3
