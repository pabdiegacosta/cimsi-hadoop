---
# General
# Definición de usuario, grupo y contraseña así como el directorio home
user: "hadoop"
group: "hadoop"
home_dir: "/home/{{ user }}"
pass: "cimsi"

# Dependencias de apt y pip
os_dependencies:
  - openjdk-8-jdk
  #- python3.8
  #- python3-pip
  - python3-numpy
  - python3-pandas
  - python3-matplotlib
  - sshpass
#python_dependencies:
#  - numpy
#  - pandas
#  - matplotlib

java_home: "/usr/lib/jvm/java-8-openjdk-amd64"


# Ansible
# Contraseña para la conexión SSH y become de Ansible
ansible_user: "{{ user }}"
ansible_ssh_pass: "{{ pass }}"
ansible_become_password: "{{ pass }}"
ansible_ssh_common_args: "-o StrictHostKeyChecking=no" # Para evitar tener que confirmar la conexión SSH por primera vez


# Haddoop
# Definición de la versión de Hadoop, enlace de descarga y directorio de instalación
hadoop_tar_filename: "hadoop-3.3.5"
hadoop_download_link: "https://downloads.apache.org/hadoop/common/{{ hadoop_tar_filename }}/{{ hadoop_tar_filename }}.tar.gz"
hadoop_tar_path: "{{ home_dir }}/{{ hadoop_tar_filename }}.tar.gz"
hadoop_install_path: "{{ home_dir }}"

# Usuario y grupo de Hadoop, directorio home de instalación de Hadoop
hadoop_user: "{{ user }}"
hadoop_group: "{{ group }}"
hadoop_home: "{{ hadoop_install_path }}/hadoop"

# Ruta de archivos de configuración
hadoop_conf_dir: "{{ hadoop_home }}/etc/hadoop"

# HDFS
# Host, puerto y usuarios de namenode y datanodes
hdfs_namenode_host: "{{ hostvars['master']['name'] }}"
hdfs_namenode_ip: "{{ hostvars['master']['ansible_host'] }}"
hdfs_namenode_port: 9000
hdfs_namenode_user: "{{ hadoop_user }}"
hdfs_datanode_user: "{{ hadoop_user }}"
# Directorios de HDFS para almacenar el sistema de archivos y directorio temporal
hdfs_name_dir: "{{ home_dir }}/hdfs/name"
hdfs_namesecondary_dir: "{{ home_dir }}/hdfs/namesecondary"
hdfs_data_dir: "{{ home_dir }}/hdfs/data"
hdfs_temp_dir: "{{ home_dir }}/hdfs/tmp"
# Ajustes de replicación y failover
hdfs_dfs_replication: 2
hdfs_ha_automatic_failover_enabled: "true"


# Ajustes de MapReduce (En principio no se usará, pues Spark es el motor de procesamiento)
yarn_app_mapreduce_am_resource_mb: 1024
mapreduce_map_memory_mb: 512
mapreduce_reduce_memory_mb: 512

# Ajustes de YARN
yarn_resourcemanager_host: "{{ hdfs_namenode_host }}"
yarn_resourcemanager_ip: "{{ hdfs_namenode_ip }}"
yarn_nodemanager_resource_memory_mb: 3072
yarn_scheduler_maximum_allocation_mb: 3072
yarn_scheduler_minimum_allocation_mb: 256
yarn_nodemanager_vmem_check_enabled: "false"
yarn_new_ui: "true"


# Spark
# Definición de la versión de Spark, enlace de descarga y directorio de instalación
spark_tar_filename: "spark-3.5.3-bin-hadoop3"
spark_download_link: "https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz"
spark_tar_path: "{{ home_dir }}/{{ spark_tar_filename }}.tgz"
spark_install_path: "{{ home_dir }}"

# Configuración de Spark
# Directorio home de instalación de Spark 
spark_home: "{{ hadoop_install_path }}/spark"
# Modo de despliegue
spark_master: "yarn"
spark_deploy_mode: "cluster"
# Configuración del driver
spark_driver_memory: '1G'
spark_driver_cores: 1
# Configuración del ejecutor
spark_executor_memory: '2G'
spark_executor_cores: 1
spark_executor_instances: 3
# Configuración de overhead y paralelismo
spark_yarn_executor_memoryOverhead: '256m'
spark_dynamic_allocation_enabled: "false"
spark_default_parallelism: 6
# Configuración de logs e historial
spark_eventlog_dir: "hdfs://{{hdfs_namenode_host}}:{{hdfs_namenode_port}}/spark-logs"
spark_history_fs_update_interval: "10s"
spark_history_ui_port: 18080


# Carpetas de HDFS para almacenar los datos de entrada y salida de la tarea Word2Vec
hdfs_base_path: "/user/{{ hadoop_user }}"
hdfs_input_path: "{{ hdfs_base_path }}/word2vec/input/"
hdfs_output_path: "{{ hdfs_base_path }}/word2vec/output/"
#input_text: "text2.txt"
#output_csv: "text2.csv"
#output_image: "text2.png"
