---
# tasks file for spark-script

- name: Check if provided input text exists in the HDFS input path
  command: "{{ hadoop_home }}/bin/hdfs dfs -test -f {{ hdfs_input_path }}/{{ input_text }}"
  tags: [ check_input_text ]

- name: Copy PySpark script
  template:
    src: word2vec_job.py.j2
    dest: "{{ home_dir }}/word2vec_job.py"
  tags: [ copy_pyspark_script ]

- name: Submit the PySpark job to the Spark cluster
  shell:  "{{ spark_home }}/bin/spark-submit {{ home_dir }}/word2vec_job.py"
  environment:
    HADOOP_CONF_DIR: "{{ hadoop_conf_dir }}"
  args:
    executable: /bin/bash
  register: spark_job_output
  tags: [ submit_pyspark_job ]

- name: Show Spark job output
  debug:
    msg: "{{ spark_job_output.stdout }}"
  tags: [ show_spark_job_output ]

- name: Show Spark job error if it failed
  fail:
    msg: "PySpark job failed: {{ spark_job_output.stderr }}"
  when: spark_job_output.rc != 0
  tags: [ show_spark_job_output ]

- name: Copy the output image to the host machine
  fetch:
    src: "{{ home_dir }}/{{ output_image }}"
    dest: "./{{ output_image }}"
    flat: yes
  tags: [ fetch_output_image ]
